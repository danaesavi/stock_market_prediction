\chapter{Introduction}
\label{ch:intro}

\begin{chapterquote}{Oscar Wilde}
	Where there is no love, there is no understanding.
\end{chapterquote}
Around the world, computers capture and store terabytes of data everyday. for example, banks are building up pictures of how people spend their money, hospitals are recording, what treatments patients are on for which aliments, and engine monitoring systems in care record information about the engine in order to detect when it might fail. The real challenge is to do something useful with this data. 
The size and complexity of these datasets means that humans are unable to extract useful information from them. Even the way the data is stored works against us. We would need to reduce the number of dimensions until our rain can deal with the problem. 
This is one reason why machine learning is becoming so popular.
[MLA]\\
---------------------------------------------------------------


This works is focused on the construction of a model that imitate a specific and limited behavior of humans: generation of a natural language such as English or Spanish. We would like a program able to follow input sentences just like an average human being would. One way to achieve this task is to program all the syntactic and semantic rules of the language into one very large program. Another way is to construct a skeletal program that starts with a limited ability to understand the language and then understands the rules of it from experience just as humans use to do[MLT]. This work is interested in the second alternative[MLT].\\
---------------------------------------------------------------


\subsection{N-grams}
The standard approach to statistical modelling of language was based on counting frequencies of occurrences of short symbol sequences of length up to N (called N-grams). The number of possible N-grams is on the order of VN, where V is the vocabulary size N-grams treat each word as an atomic unit, so they cannot generalize across semantically related sequences of words, whereas neural language models can because they associate each word with a vector of real valued features, and semantically related words end up close to each other in that vector space (Fig. 4).\cite{lecun2015deep}

\subsection{Prediction by Partial Matching}
The prediction by partial algorithms are compression algorithms where the prediction is determined by counting exact matches between the recent history and the training set \cite{graves2013generating}.
    
\subsection{Neural Language Models}
Before the introduction of neural language model the standard approach to statistical modelling of language did not exploit distributed representations\cite{lecun2015deep}

Recurrent Neural Networks are dynamic models that have been used to generate sequences in different domains such as music, text and motion capture data \cite{graves2013generating}.
They also have been found to be very good at predicting the next character in the text or the next word in a sequence, since the hidden layers of a multilayer neural network learn to represent the networkâ€™s inputs in a way that makes it easy to predict the target outputs.\cite{lecun2015deep}

\section{First Section}