\chapter{Introduction}
\label{ch:intro}

\begin{chapterquote}{Leslie Lamport}
	Formal mathematics is nature's way of letting you know how sloppy
your mathematics is.
\end{chapterquote}

Este trabajo presenta una plantilla para las tesis y tesinas del Instituto Tecnológico Autónomo de México para los usuarios de \LaTeX \cite{lamport1994latex}. Nace de la necesidad de los matemáticos, actuarios e ingenieros (entre otras carreras) por utilizar un sistema de composición de textos adecuado para su trabajo de titulación. El objetivo es ayudar a la comunidad del ITAM a simplificar el proceso de escritura y edición de sus tesis, tesinas o casos. A continuación describimos a mayor detalle cada una de las partes de la plantilla.

\subsection{Natural Language Processing}
	\subsubsection{N-grams}
The standard approach to statistical modelling of language was based on counting frequencies of occurrences of short symbol sequences of length up to N (called N-grams). The number of possible N-grams is on the order of VN, where V is the vocabulary size N-grams treat each word as an atomic unit, so they cannot generalize across semantically related sequences of words, whereas neural language models can because they associate each word with a vector of real valued features, and semantically related words end up close to each other in that vector space (Fig. 4).\cite{lecun2015deep}

    \subsubsection{Prediction by Partial Matching}
The prediction by partial algorithms are compression algorithms where the prediction is determined by counting exact matches between the recent history and the training set \cite{graves2013generating}.
    
    \subsubsection{Neural Language Models}
Before the introduction of neural language model the standard approach to statistical modelling of language did not exploit distributed representations\cite{lecun2015deep}

\subsection{Machine Learning}

\subsection{Neural Networks}
After training, the performance of the system is measured on a different set of examples called a test set. This serves to test the generalization ability of the machine\cite{lecun2015deep}.
\subsubsection{Backpropagation}
	The weight vector is formed by the internal adjustable parameters which are adjusted to reduce the distance between the output scores and the desired pattern of the scores. To adjust the weight vector, for each weight, indicates by what amount the error would increase or decrease if the weight were increased by a tiny amount. The weight vector is then adjusted in the opposite direction to the gradient vector\cite{lecun2015deep}.

it was commonly thought that simple gradient descent would get trapped in poor local
minima In practice, poor local minima are rarely a problem with large networks.\cite{lecun2015deep}



\subsection{Deep Learning}
Representation learning is a set of methods that allows a machine to be fed with raw data and to automatically discover the representations needed for detection or classification. Deep-learning methods are representation-learning methods with multiple levels of representation. With the composition of enough such transformations, very complex functions can be learned. For classification tasks, higher layers of representation amplify aspects of the input that are important for discrimination and suppress irrelevant variations\cite{lecun2015deep}.


\subsection{Recurrent Neural Networks}
A recurrent neural network is a artificial neural network model.The difference of its structure is that connections among hidden units associated with a time delay are allowed, enabling the model to retain information about the past inputs. In this way, temporal correlations between events that are possibly far away from each other in the data can be discovered \cite{pascanu2013difficulty}. 

The network is `deep' in both space and time, in the sense that every piece of information passing either vertically or horizontally through the computation graph will be acted on by multiple successive weight matrices and nonlinearities \cite{graves2013generating}.

RNNs are fuzzy in the sense that they use their internal representation to perform a high-dimensional interpolation between training examples so they do not use exact temples from the training data to make predictions \cite{graves2013generating}
The result is that rnns synthesise and reconstitute the training data in a complex way, and rarely generate the same thing twice. Furthermore, fuzzy predictions do not suffer from the curse of dimensionality, and are therefore much better at modelling real-valued or multivariate data than exact matches  \cite{graves2013generating}.

For tasks that involve sequential inputs, such as speech and language, it results better to use RNNs. RNNs process an input sequence one element at a time, maintaining in their hidden units a ‘state vector’ that implicitly contains information about the history of all the past elements of the sequence.\cite{lecun2015deep}


\subsubsection{Backpropagation Through Time}

RNNs, once unfolded in time (Fig. 5), can be seen as very deep feedforward networks in which all the layers share the same weights\cite{lecun2015deep}
One approach to compute the gradients is Backpropagation Through Time (BPTT). Here, the model is presented as a deep multi-layer one where backpropagation is applied on the unrolled model\cite{pascanu2013difficulty}.
The backpropagation procedure to compute the gradient of an objective function with respect to the weights of a multilayer stack of modules is nothing more than a practical application of the chain rule for derivatives. The key insight is that the derivative (or gradient)
of the objective with respect to the input of a module can be computed by working backwards from the gradient with respect to the output of that module (or the input of the subsequent module). The backpropagation equation can be applied repeatedly to propagate gradients through all modules, starting from the output at the top (where the network produces its prediction) all the way to the bottom (where the external input is fed). Once these gradients have been computed, it is straightforward to compute the gradients with respect to the weights of each module. To go from one layer to the next, a set of units compute a weighted sum of their inputs from the previous layer and pass the result through a non-linear function \cite{lecun2015deep}

\subsubsection{RNS Training Issues}
RNNs are very powerful dynamic systems, but training them has proved to be problematic because the backpropagated gradients either grow or shrink at each time step, so over many time steps they typically explode or vanish\cite{lecun2015deep}

The exploding gradients problem refers to the large increase in the norm of the gradient during training caused by the explosion of the long term components. On the other hand, the vanishing gradients problem is presented when long term components go exponentially fast to norm 0 making impossible for the model to learn correlation between temporally distant events \cite{pascanu2013difficulty}. 

\begin{enumerate}
\item The exploding gradients problem \\
 One simple mechanism to deal with the exploding gradients problem is to rescale them whenever they go over a threshold. One good heuristic for setting this threshold is to look at statistics on the average norm over a sufficiently large number of updates. In this way, we can handle very abrupt changes in norm, so the algorithm can be thought of as adapting the learning rate based on the norm of the gradient  \cite{pascanu2013difficulty}.
Algorithm
\item The vanishing gradients problem \\
The vanshing gradients problem makes it difficult to learn to store information for very long. 
This amnesia makes them prone to instability when generating sequences. The problem is that if the network's predictions  are only based on the last few inputs, and these inputs were themselves predicted by the network, it has little opportunity to recover from past mistakes. Having a longer memory has a establishing effect, because even if the network cannot make sense of its recent history, it can look further back in the past to formulate its predictions. \cite{graves2013generating}

A solution proposed for the vanishing gradient problem is to use a regularization term that forces the error signal not to vanish as it travels back in time  write the algorithm 1 and the regularization term \cite{pascanu2013difficulty}.
A second idea is to augment the network with an explicit memory. The first proposal of this kind is the long short-term memory (LSTM) networks.

\end{enumerate}
\subsubsection{Long Short-Term Memory Networks}
Long Short-term Memory (LSTM) is an RNN architecture designed to be better at storing and accessing information than standard RNNs \cite{graves2013generating}.
Although in most RNNs the hidden layer function H is an elementwise application of sigmoid function.,  Long Short-Term Memory  architecture uses purpose-built memory cells to store information, is better at finding and exploiting long range dependencies in the data. It has a input gate, forget gate, output gate, cell and cell input activation vectors, all of which are the same size as the hidden vector h. \cite{graves2013generating}
The memory cell (hay que ver cual de las 3 es) acts like an accumulator or a gated leaky neuron: it has a connection to itself at the next time step that has a weight of one, so it copies its own real-valued state and accumulates the external signal, but this self-connection is multiplicatively gated by another unit that learns to decide when to clear the content of the memory \cite{lecun2015deep}.

LSTM networks have subsequently proved to be more effective than conventional RNNs, especially when they have several layers for each time step\cite{lecun2015deep}.
The full gradient can be calculated with backpropagation through time \cite{graves2013generating}
This model has been used in several applications such as speech and handwriting recognition.\cite{graves2013generating}



\section{Language Modelling}
Recurrent Neural Networks are dynamic models that have been used to generate sequences in different domains such as music, text and motion capture data \cite{graves2013generating}.
They also have been found to be very good at predicting the next character in the text or the next word in a sequence, since the hidden layers of a multilayer neural network learn to represent the network’s inputs in a way that makes it easy to predict the target outputs.\cite{lecun2015deep}

Novel sequences can be generated by by a trained network by iteratively sampling from the network's output distribution, assuming the predictions are probabilistic and then feeding in the sample as input at the next step, meaning by making the network treats its inventions as if they were real. 
\cite{graves2013generating}.


\subsection{Words Representation}
Text data is discrete, and is typically presented to neural networks using `onehot' input vectors. That is, if there are K text classes in total, and class k is fed in at time t, then xt is a length K vector whose entries are all zero except for the kth, which is one. Pr(xt+1jyt) is therefore a multinomial distribution, which can be naturally parameterised by a softmax function at the output layer:
formulas
\begin{enumerate}
\item Word-level language modelling \\
In most cases, text prediction  is performed at the word level. K is therefore the number of words in the dictionary. \cite{graves2013generating}
In the first layer, each word creates a different pattern of activations, or word vectors (Fig. 4). In a language model, the other layers of the network learn to convert the input word vectors into an output word vector for the predicted next word, which can be used to predict the probability for any word in the vocabulary. Learning word vectors turned out to also work very well when the word sequences come from a large corpus of real text and the individual micro-rules are unreliable.\cite{lecun2015deep}

This can be problematic for realistic tasks, where the number of words often exceeds 100,000. As well as requiring many parameters to model, having so many classes demands a huge amount of training data to adequately cover the possible contexts for the words. A further diculty is the high computational cost of evaluating all the exponentials during training. Furthermore, word-level models are not applicable to text data containing non-word strings, such as multi-digit numbers or web addresses.\cite{graves2013generating}

\item Character-level language modelling \\
Character-level language modelling has been found to give slightly worse performance than equivalent word-level models. Nonetheless, predicting one character at a time is more interesting from the perspective of sequence generation, because it allows the network to invent novel words and strings.
\cite{graves2013generating}


\end{enumerate}